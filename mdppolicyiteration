import numpy as np

# Define the grid world
grid_rows = 3
grid_cols = 4
terminal_states = [(2, 3), (1, 3)]
rewards = np.full((grid_rows, grid_cols), -0.04)
rewards[2, 3] = 1.0
rewards[1, 3] = -1.0
obstacles = [(1, 1)]

# Define actions
actions = ['U', 'D', 'L', 'R']
action_to_delta = {
    'U': (-1, 0),
    'D': (1, 0),
    'L': (0, -1),
    'R': (0, 1)
}

# Transition probabilities
def transition_probs(state, action):
    probs = []
    if state in terminal_states or state in obstacles:
        return [(0, state)]

    intended_move = action_to_delta[action]
    perpend_moves = [action_to_delta[a] for a in actions if a != action]

    for move, prob in [(intended_move, 0.8), (perpend_moves[0], 0.1), (perpend_moves[1], 0.1)]:
        new_state = (state[0] + move[0], state[1] + move[1])
        if 0 <= new_state[0] < grid_rows and 0 <= new_state[1] < grid_cols and new_state not in obstacles:
            probs.append((prob, new_state))
        else:
            probs.append((prob, state))
    return probs

# Initialize policy and value function
policy = np.full((grid_rows, grid_cols), 'U')
value_func = np.zeros((grid_rows, grid_cols))

# Policy Iteration parameters
gamma = 0.99
theta = 1e-4

# Policy Evaluation
def policy_evaluation(policy, value_func):
    while True:
        delta = 0
        for r in range(grid_rows):
            for c in range(grid_cols):
                state = (r, c)
                if state in terminal_states or state in obstacles:
                    continue
                v = value_func[state]
                new_value = 0
                for prob, next_state in transition_probs(state, policy[state]):
                    new_value += prob * (rewards[next_state] + gamma * value_func[next_state])
                value_func[state] = new_value
                delta = max(delta, abs(v - new_value))
        if delta < theta:
            break
    return value_func

# Policy Improvement
def policy_improvement(policy, value_func):
    policy_stable = True
    for r in range(grid_rows):
        for c in range(grid_cols):
            state = (r, c)
            if state in terminal_states or state in obstacles:
                continue
            old_action = policy[state]
            action_values = {}
            for action in actions:
                action_value = 0
                for prob, next_state in transition_probs(state, action):
                    action_value += prob * (rewards[next_state] + gamma * value_func[next_state])
                action_values[action] = action_value
            best_action = max(action_values, key=action_values.get)
            policy[state] = best_action
            if old_action != best_action:
                policy_stable = False
    return policy, policy_stable

# Policy Iteration
while True:
    value_func = policy_evaluation(policy, value_func)
    policy, policy_stable = policy_improvement(policy, value_func)
    if policy_stable:
        break

# Display the results
print("Optimal Policy:")
for row in policy:
    print(" ".join(row))

print("\nValue Function:")
for row in value_func:
    print(" ".join(f"{val: .2f}" for val in row))
